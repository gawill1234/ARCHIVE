<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
<HEAD>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<META name="date" content="2004-09-07T12:21:57+00:00">
</HEAD>
<BODY>
<STYLE type="text/css">
.v {WIDTH: 1em; WRITING-MODE: tb-rl}
</STYLE><!-- Page 1 -->
<a name="1"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft0{font-size:21px;font-family:Times;color:#000000;}
	.ft1{font-size:16px;font-family:Times;color:#000000;}
	.ft2{font-size:21px;line-height:20px;font-family:Times;color:#000000;}
	.ft3{font-size:16px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:275;left:135"><nobr><span class="ft2">An Introduction to Heritrix<br>An open source archival quality web crawler</span></nobr></DIV>
<DIV style="position:absolute;top:343;left:135"><nobr><span class="ft1">July 14, 2004</span></nobr></DIV>
<DIV style="position:absolute;top:488;left:135"><nobr><span class="ft3">Internet Archive Web Team<br>Gordon Mohr<br>Michael Stack<br>Igor Ranitovic<br>Dan Avery<br>Michele Kimpton</span></nobr></DIV>
</DIV>
<!-- Page 2 -->
<a name="2"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft4{font-size:21px;line-height:25px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:246;left:135"><nobr><span class="ft0">Abstract</span></nobr></DIV>
<DIV style="position:absolute;top:291;left:135"><nobr><span class="ft3">Heritrix is the Internet Archive's open-source, extensible, web-scale, archival-quality<br>webcrawler project.  The Internet Archive started Heritrix development in the early part<br>of 2003.   The intention was to develop a crawler for the specific purpose of archiving<br>websites and to support multiple different use cases including focused and broadcrawling.<br>The software is open source to encourage collaboration and joint development across<br>institutions with similar needs.  A pluggable, extensible architecture facilitates<br>customization and outside contribution.  Now, after over a year of development, the<br>Internet Archive and other institutions are using Heritrix to perform focused and<br>increasingly broad crawls.</span></nobr></DIV>
<DIV style="position:absolute;top:496;left:135"><nobr><span class="ft2">Introduction<br>The Internet Archive (IA) is a 5013C non-profit corporation, whose mission is to build a<br>public Internet digital library.  Over the last 6 years, IA has built the largest public web<br>archive to date, hosting over 400 TB of data.</span></nobr></DIV>
<DIV style="position:absolute;top:610;left:135"><nobr><span class="ft3">The Web Archive is comprised primarily of pages collected by Alexa Internet starting in<br>1996.  Alexa Internet is a Web cataloguing company founded by Brewster Kahle and<br>Bruce Gilliat in 1996. Alexa Internet takes a snapshot of the web every 2 months,<br>currently collecting 10 TB of data per month from over 35 million sites.  Alexa Internet<br>donates this crawl to the Internet Archive, and IA stores and indexes the collection. Alexa<br>uses its own proprietary software and techniques to crawl the web. This software is not<br>available to Internet Archive or other institutions for use or extension.</span></nobr></DIV>
<DIV style="position:absolute;top:776;left:135"><nobr><span class="ft3">In the latter part of 2002, the Internet Archive wanted the ability to do crawling internally<br>for its own purposes and to be able to partner with other institutions to crawl and archive<br>the web in new ways.  The Internet Archive concluded it needed a large-scale, thorough,<br>easily customizable crawler. After doing an evaluation of other open source software<br>available at the time it was concluded no appropriate software existed that had the<br>flexibility required yet could scale to perform broad crawls.</span></nobr></DIV>
<DIV style="position:absolute;top:921;left:135"><nobr><span class="ft3">The Internet Archive believed it was essential the software be open source to promote<br>collaboration between institutions interested in archiving the web.   Developing open<br>source software would encourage participating institutions to share crawling experiences,<br>solutions to common problems, and even the development of new features.</span></nobr></DIV>
<DIV style="position:absolute;top:1024;left:135"><nobr><span class="ft3">The Internet Archive began work on this new open source crawler development project in<br>the first half of 2003.  It named the crawler Heritrix.  This paper gives a high-level</span></nobr></DIV>
</DIV>
<!-- Page 3 -->
<a name="3"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:109;left:135"><nobr><span class="ft3">overview of the Heritrix crawler, circa version 1.0.0 (August, 2004).  It outlines the<br>original use-cases, , the general architecture, current capabilities and current limitations.<br>It also describes how the crawler is currently used at the Internet Archive and future plans<br>for development both internally and by partner institutions.  It also describes how the<br>Archive currently uses the crawler and future plans for development both internally and<br>by partner institutions.</span></nobr></DIV>
<DIV style="position:absolute;top:251;left:135"><nobr><span class="ft0">Use Cases</span></nobr></DIV>
<DIV style="position:absolute;top:283;left:189"><nobr><span class="ft1">The Internet Archive and its collaborators wanted a crawler capable of each of the</span></nobr></DIV>
<DIV style="position:absolute;top:304;left:135"><nobr><span class="ft1">following crawl types:</span></nobr></DIV>
<DIV style="position:absolute;top:345;left:135"><nobr><span class="ft3">Broad crawling: Broad crawls are large, high-bandwidth crawls in which the number of<br>sites and the number of valuable individual pages collected is as important as the<br>completeness with which any one site is covered.  At the extreme, a broad crawl tries to<br>sample as much of the web as possible given the time, bandwidth, and storage resources<br>available.</span></nobr></DIV>
<DIV style="position:absolute;top:469;left:135"><nobr><span class="ft3">Focused crawling: Focused crawls are small- to medium-sized crawls (usually less than<br>10 million unique documents) in which the quality criterion is complete coverage of<br>some selected sites or topics.</span></nobr></DIV>
<DIV style="position:absolute;top:552;left:135"><nobr><span class="ft3">Continuous crawling: Traditionally, crawlers pursue a snapshot of resources of interest,<br>downloading each unique URI one time only.  Continuous crawling, by contrast, revisits<br>previously fetched pages – looking for changes – as well as discovering and fetching new<br>pages, even adapting its rate of visitation based on operator parameters and estimated<br>change frequencies.</span></nobr></DIV>
<DIV style="position:absolute;top:676;left:135"><nobr><span class="ft3">Experimental crawling: The Internet Archive and other groups want to experiment with<br>crawling techniques in areas such as choice of what to crawl, order in which resources are<br>crawled, crawling using diverse protocols, and analysis and archiving of crawl results.</span></nobr></DIV>
<DIV style="position:absolute;top:757;left:135"><nobr><span class="ft2">Required Capabilities<br>Based on the above use cases, the Internet Archive compiled a list of the capabilities<br>required of a crawling program. An important contribution in developing the archival<br>crawling requirements came from the efforts of the International Internet Preservation<br>Consortium (IIPC) a consortium of twelve National Libraries and the Internet Archive.<br>The mission of the IIPC is to acquire, preserve and make accessible knowledge and<br>information from the Internet for future generations everywhere, promoting global<br>exchange and international relations.  IIPC member libraries have diverse web resource<br>collection needs, and contributed several detailed requirements that helped define the<br>goals for a common crawler. The detailed requirements document developed by the IIPC<br>can be found at:<br>&quot;http://netpreserve.org/publications/iipc001.pdf&quot;http://netpreserve.org/publications/iipc-<br>d-001.pdf.</span></nobr></DIV>
</DIV>
<!-- Page 4 -->
<a name="4"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:109;left:135"><nobr><span class="ft2">The Heritrix Project<br>Heritrix is an archaic word for heiress, a woman who inherits. Since our crawler seeks to<br>collect and preserve the digital artifacts of our culture for the benefit of future researchers<br>and generations, this name seemed appropriate.<br>Java was chosen as the implementation software language.  As a high-level, object-<br>oriented language, Java offers strong support for modular design and components that are<br>both incrementally extendable and individually replaceable. Other key reasons for<br>choosing Java were its rich set of quality open source libraries and its large developer<br>community, making it more likely that we would benefit from previous work and outside<br>contributions.<br>The project homepage is &lt;http://crawler.archive.org&gt;, featuring the most current<br>information on the project, downloads of the crawling software, and project documents.<br>There are also public databases of outstanding feature requests and bugs.  The project<br>also provides an open mailing list to promote exchange of information between Heritrix<br>developers and other interested users.<br>Sourceforge [SOURCEFORGE], a site offering free online services for over 84,000 open<br>source software efforts, hosts the Heritrix project. Sourceforge provides many of the<br>online collaborative tools required to manage distributed-team software projects such as:</span></nobr></DIV>
<DIV style="position:absolute;top:513;left:135"><nobr><span class="ft3">Versioned source code repository (CVS)<br>Issue databases for tracking bugs and enhancement requests<br>Web hosting<br>Mirrored, archived, high-availability file release system<br>Mailing lists</span></nobr></DIV>
<DIV style="position:absolute;top:648;left:135"><nobr><span class="ft3">A large community of developers knows and uses Sourceforge, which can help to create<br>awareness of and interest in the software. The limitations of Sourceforge include:</span></nobr></DIV>
<DIV style="position:absolute;top:710;left:135"><nobr><span class="ft3">Occasionally unavailable or slow<br>No direct control over problems that do arise<br>Issue-tracking features and reporting are crude<br>Can not be used to manage other internal software projects which may not be open source</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:135"><nobr><span class="ft3">Heritrix is licensed under the Gnu Lesser General Public License (LGPL) [LGPL].  The<br>LGPL is similar to the Gnu General Public License (GPL) in that it offers free access to<br>the full source code of a program for reuse, extension, and the creation of derivative<br>works, under the condition that changes to the code are also made freely available.<br>However, it differs from the full GPL in allowing use as a module or library inside other<br>proprietary, hidden source applications, as long as changes to the library are shared.</span></nobr></DIV>
<DIV style="position:absolute;top:977;left:135"><nobr><span class="ft4">Milestones<br>Since project inception, major project milestones have included:</span></nobr></DIV>
<DIV style="position:absolute;top:1050;left:135"><nobr><span class="ft1">-Investigational crawler prototype created, and various threading and network access</span></nobr></DIV>
</DIV>
<!-- Page 5 -->
<a name="5"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft5{font-size:9px;font-family:Times;color:#000000;}
	.ft6{font-size:16px;line-height:29px;font-family:Times;color:#000000;}
	.ft7{font-size:16px;line-height:22px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:109;left:135"><nobr><span class="ft1">strategies tested, 2</span></nobr></DIV>
<DIV style="position:absolute;top:106;left:267"><nobr><span class="ft5">nd</span></nobr></DIV>
<DIV style="position:absolute;top:109;left:279"><nobr><span class="ft1"> Q 2003</span></nobr></DIV>
<DIV style="position:absolute;top:138;left:135"><nobr><span class="ft3">-Core crawler without user-interface created, to verify architecture and test coverage<br>compared to HTTrack [HTTRACK] and Mercator</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:500"><nobr><span class="ft1">[MERCATOR] crawlers, 3</span></nobr></DIV>
<DIV style="position:absolute;top:156;left:696"><nobr><span class="ft5">rd</span></nobr></DIV>
<DIV style="position:absolute;top:159;left:705"><nobr><span class="ft1"> Q 2003</span></nobr></DIV>
<DIV style="position:absolute;top:188;left:135"><nobr><span class="ft1">-Nordic Web Archive [NWA] programmers join project in San Francisco, 4</span></nobr></DIV>
<DIV style="position:absolute;top:186;left:681"><nobr><span class="ft5">th</span></nobr></DIV>
<DIV style="position:absolute;top:188;left:691"><nobr><span class="ft1"> Q 2003 –</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:135"><nobr><span class="ft1">1</span></nobr></DIV>
<DIV style="position:absolute;top:206;left:144"><nobr><span class="ft5">st</span></nobr></DIV>
<DIV style="position:absolute;top:209;left:152"><nobr><span class="ft1"> Q 2004, adding a web user-interface, a rich configuration framework, documentation,</span></nobr></DIV>
<DIV style="position:absolute;top:230;left:135"><nobr><span class="ft6">and other key improvements<br>-First public release (version 0.2.0) on Sourceforge, January 2004<br>-First contributions by unaffiliated developer, January 2004<br>-Workshops with National Library users in February and June of 2004<br>-Used as the IA’s crawler for all contracted crawls – usually consisting of a few dozen to<br>a few hundred sites of news, cultural, or public-policy interest – since beginning of 2004.<br>-Adopted as the official crawler for the NWA, June 2004<br>-Version 1.0.0 official release, for focused and experimental crawling, in August 2004</span></nobr></DIV>
<DIV style="position:absolute;top:514;left:135"><nobr><span class="ft2">Heritrix Crawler Architecture<br>In this section we give an overview of the Heritrix architecture, describing the general<br>operation and key components.</span></nobr></DIV>
<DIV style="position:absolute;top:607;left:135"><nobr><span class="ft3">At its core, the Heritrix crawler was designed as a generic crawling framework into<br>which various interchangeable components can be plugged. Varying these components<br>enables diverse collection and archival strategies, and supports the incremental evolution<br>of the crawler from limited features and small crawls to our ultimate goal of giant full-<br>featured crawls.</span></nobr></DIV>
<DIV style="position:absolute;top:732;left:135"><nobr><span class="ft3">Crawl setup involves choosing and configuring a set of specific components to run.<br>Executing a crawl repeats the following recursive process, common to all web crawlers,<br>with the specific components chosen:</span></nobr></DIV>
<DIV style="position:absolute;top:814;left:149"><nobr><span class="ft3">1. Choose a URI from among all those scheduled<br>2. Fetch that URI<br>3. Analyze or archive the results<br>4. Select discovered URIs of interest, and add to those scheduled<br>5. Note that the URI is done and repeat</span></nobr></DIV>
<DIV style="position:absolute;top:940;left:135"><nobr><span class="ft7">The three most prominent components of Heritrix are the Scope, the Frontier, and the<br>Processor Chains, which together serve to define a crawl.</span></nobr></DIV>
<DIV style="position:absolute;top:994;left:135"><nobr><span class="ft7">The Scope determines what URIs are ruled into or out of a certain crawl. The Scope<br>includes the &quot;seed&quot; URIs used to start a crawl, plus the rules used in step 4 above to<br>determine which discovered URIs are also to be scheduled for download.</span></nobr></DIV>
</DIV>
<!-- Page 6 -->
<a name="6"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft8{font-size:15px;font-family:Times;color:#000000;}
	.ft9{font-size:13px;font-family:Times;color:#000000;}
	.ft10{font-size:10px;font-family:Times;color:#000000;}
	.ft11{font-size:11px;font-family:Times;color:#000000;}
	.ft12{font-size:10px;line-height:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:110;left:135"><nobr><span class="ft7">The Frontier tracks which URIs are scheduled to be collected, and those that have already<br>been collected. It is responsible for selecting the next URI to be tried (in step 1 above),<br>and prevents the redundant rescheduling of already-scheduled URIs (in step 4 above).</span></nobr></DIV>
<DIV style="position:absolute;top:187;left:135"><nobr><span class="ft7">The Processor Chains include modular Processors that perform specific, ordered actions<br>on each URI in turn. These include fetching the URI (as in step 2 above), analyzing the<br>returned results (as in step 3 above), and passing discovered URIs back to the Frontier (as<br>in step 4 above).</span></nobr></DIV>
<DIV style="position:absolute;top:286;left:135"><nobr><span class="ft7">Figure 1 shows these major components of the crawler, as well as other supporting<br>components, with major relationships highlighted.</span></nobr></DIV>
<DIV style="position:absolute;top:349;left:135"><nobr><span class="ft1">Figure 1: Major Components of Heritrix in a Representative Configuration</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:484"><nobr><span class="ft9">CrawlOrder</span></nobr></DIV>
<DIV style="position:absolute;top:470;left:148"><nobr><span class="ft9">CrawlController</span></nobr></DIV>
<DIV style="position:absolute;top:589;left:343"><nobr><span class="ft10">ServerCache</span></nobr></DIV>
<DIV style="position:absolute;top:704;left:343"><nobr><span class="ft10">Scope</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:469"><nobr><span class="ft12">Prefetch Chain<br>? Preselector<br>? PreconditionEnforcer</span></nobr></DIV>
<DIV style="position:absolute;top:386;left:148"><nobr><span class="ft9">Web Administrative Console</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:176"><nobr><span class="ft9">Frontier</span></nobr></DIV>
<DIV style="position:absolute;top:735;left:190"><nobr><span class="ft12">Already<br>Included URIs</span></nobr></DIV>
<DIV style="position:absolute;top:567;left:190"><nobr><span class="ft12">URI Work<br>Queues</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:469"><nobr><span class="ft12">Fetch Chain<br>? FetchDNS<br>? FetchHTTP</span></nobr></DIV>
<DIV style="position:absolute;top:685;left:469"><nobr><span class="ft12">Extractor Chain<br>? ExtractorHTML<br>? ExtractorJS</span></nobr></DIV>
<DIV style="position:absolute;top:755;left:469"><nobr><span class="ft12">Write Chain<br>? ARCWriterProcessor</span></nobr></DIV>
<DIV style="position:absolute;top:811;left:469"><nobr><span class="ft12">Postprocess Chain<br>? CrawlStateUpdater<br>? Postselector</span></nobr></DIV>
<DIV style="position:absolute;top:539;left:679"><nobr><span class="ft9">ToeThreads</span></nobr></DIV>
<DIV style="position:absolute;top:525;left:665"><nobr><span class="ft9">ToeThreads</span></nobr></DIV>
<DIV style="position:absolute;top:511;left:646"><nobr><span class="ft9">ToeThreads</span></nobr></DIV>
<DIV style="position:absolute;top:486;left:427"><nobr><span class="ft10">next(CrawlURI)</span></nobr></DIV>
<DIV style="position:absolute;top:894;left:419"><nobr><span class="ft10">finished(CrawlURI)</span></nobr></DIV>
<DIV style="position:absolute;top:832;left:344"><nobr><span class="ft10">schedule(URI)</span></nobr></DIV>
<DIV style="position:absolute;top:945;left:135"><nobr><span class="ft11">CAPTION: The Web Administrative Console composes a CrawlOrder, which is then used to create a working</span></nobr></DIV>
<DIV style="position:absolute;top:968;left:135"><nobr><span class="ft11">assemblage of components within the CrawlController. Within the CrawlController, arrows indicate the progress of a</span></nobr></DIV>
<DIV style="position:absolute;top:990;left:135"><nobr><span class="ft11">single scheduled CrawlURI within one ToeThread’s execution loop. A CrawlURI is provided by the Frontier via the</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:135"><nobr><span class="ft11">next() operation, then presented to each Processor in turn, and finally returned to the Frontier via a finished()</span></nobr></DIV>
<DIV style="position:absolute;top:1035;left:135"><nobr><span class="ft11">operation. At the Postselector step, any newly discovered URIs of interest are inserted into the Frontier via a schedule()</span></nobr></DIV>
<DIV style="position:absolute;top:1058;left:135"><nobr><span class="ft11">operation.</span></nobr></DIV>
</DIV>
<!-- Page 7 -->
<a name="7"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft13{font-size:18px;font-family:Times;color:#000000;}
	.ft14{font-size:18px;line-height:25px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:108;left:135"><nobr><span class="ft14">Key Components<br>In this section we go into more detail on each of the components featured in Figure 1.</span></nobr></DIV>
<DIV style="position:absolute;top:179;left:135"><nobr><span class="ft3">The Web Administrative Console is in many ways a standalone web application, hosted<br>by the embedded Jetty Java HTTP server. Its web pages allow the operator to choose a<br>crawl's components and parameters by composing a CrawlOrder, a configuration object<br>that also has an external XML representation.</span></nobr></DIV>
<DIV style="position:absolute;top:282;left:135"><nobr><span class="ft3">A crawl is initiated by passing this CrawlOrder to the CrawlController, a component<br>which instantiates and holds references to all configured crawl components. The<br>CrawlController is the crawl's global context: all subcomponents can reach each other<br>through it. The Web Administrative Console controls the crawl through the<br>CrawlController.</span></nobr></DIV>
<DIV style="position:absolute;top:406;left:135"><nobr><span class="ft3">The CrawlOrder contains sufficient information to create the Scope. The Scope seeds the<br>Frontier with initial URIs and is consulted to decide which later-discovered URIs should<br>also be scheduled.</span></nobr></DIV>
<DIV style="position:absolute;top:489;left:135"><nobr><span class="ft3">The Frontier has responsibility for ordering the URIs to be visited, ensuring URIs are not<br>revisited unnecessarily, and moderating the crawler's visits to any one remote site. It<br>achieves these goals by maintaining a series of internal queues of URIs to be visited, and<br>a list of all URIs already visited or queued. URIs are only released from queues for<br>fetching in a manner compatible with the configured politeness policy. The default<br>provided Frontier implementation offers a primarily breadth-first, order-of-discovery<br>policy for choosing URIs to process, with an option to prefer finishing sites in progress to<br>beginning new sites. Other Frontier implementations are possible.</span></nobr></DIV>
<DIV style="position:absolute;top:675;left:135"><nobr><span class="ft3">The Heritrix crawler is multithreaded in order to make progress on many URIs in parallel<br>during network and local disk I/O lags. Each worker thread is called a ToeThread, and<br>while a crawl is active, each ToeThread loops through steps that roughly correspond to<br>the generic process outlined previously:</span></nobr></DIV>
<DIV style="position:absolute;top:779;left:135"><nobr><span class="ft3">Ask the Frontier for a next() URI<br>Pass the URI to each Processor in turn. (Distinct Processors perform the fetching,<br>analysis, and selection steps.)<br>Report the completion of the finished() URI</span></nobr></DIV>
<DIV style="position:absolute;top:882;left:135"><nobr><span class="ft3">The number of ToeThreads in a running crawler is adjustable to achieve maximum<br>throughput given local resources.  The number of ToeThreads usually ranges in the<br>hundreds.</span></nobr></DIV>
<DIV style="position:absolute;top:965;left:135"><nobr><span class="ft3">Each URI is represented by a CrawlURI instance, which packages the URI with<br>additional information collected during the crawling process, including arbitrary nested<br>named attributes. The loosely-coupled system components communicate their progress<br>and output through the CrawlURI, which carries the results of earlier processing to later<br>processors and finally, back to the Frontier to influence future retries or scheduling.</span></nobr></DIV>
</DIV>
<!-- Page 8 -->
<a name="8"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft15{font-size:16px;font-family:Times;color:#000000;}
	.ft16{font-size:16px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:129;left:135"><nobr><span class="ft3">The ServerCache holds persistent data about servers that can be shared across CrawlURIs<br>and time. It contains any number of CrawlServer entities, collecting information such as<br>IP addresses, robots exclusion policies, historical responsiveness, and per-host crawl<br>statistics.</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:135"><nobr><span class="ft3">The overall functionality of a crawler with respect to a scheduled URI is largely specified<br>by the series of Processors configured to run. Each Processor in turn performs its tasks,<br>marks up the CrawlURI state, and returns. The tasks performed will often vary<br>conditionally based on URI type, history, or retrieved content.  Certain CrawlURI state<br>also affects whether and which further processing occurs.  (For example, earlier<br>Processors may cause later processing to be skipped.)</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:135"><nobr><span class="ft1">Processors are collected into five chains:</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:135"><nobr><span class="ft3">Processors in the Prefetch Chain receive the CrawlURI before any network activity to<br>resolve or fetch the URI. Such Processors typically delay, reorder, or veto the subsequent<br>processing of a CrawlURI, for example to ensure that robots exclusion policy rules are<br>fetched and considered before a URI is processed.<br>Processors in the Fetch Chain attempt network activity to acquire the resource referred-to<br>by a CrawlURI. In the typical case of an HTTP transaction, a Fetcher Processor will fill<br>the &quot;request&quot; and &quot;response&quot; buffers of the CrawlURI, or indicate whatever error<br>condition prevented those buffers from being filled.<br>Processors in the Extract Chain perform follow-up processing on a CrawlURI for which<br>a fetch has already completed, extracting features of interest. Most commonly, these are<br>new URIs that may also be eligible for visitation. URIs are only discovered at this step,<br>not evaluated.<br>Processors in the Write Chain store the crawl results – returned content or extracted<br>features – to permanent storage. Our standard crawler merely writes data to the Internet<br>Archive's ARC file format but third parties have created Processors to write other data<br>formats or index the crawled data.<br>Finally, Processors in the Postprocess Chain perform final crawl-maintenance actions on<br>the CrawlURI, such as testing discovered URIs against the Scope, scheduling them into<br>the Frontier if necessary, and updating internal crawler information caches.</span></nobr></DIV>
<DIV style="position:absolute;top:833;left:135"><nobr><span class="ft1">Table 1: Processor modules included in Heritrix 1.0.0</span></nobr></DIV>
<DIV style="position:absolute;top:897;left:258"><nobr><span class="ft15"><b>Name</b></span></nobr></DIV>
<DIV style="position:absolute;top:897;left:413"><nobr><span class="ft15"><b>Function</b></span></nobr></DIV>
<DIV style="position:absolute;top:929;left:240"><nobr><span class="ft1">Preselector</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:413"><nobr><span class="ft3">Offers an opportunity to reject previously-<br>scheduled URIs not of interest.</span></nobr></DIV>
<DIV style="position:absolute;top:928;left:133" class="v"><nobr><span class="ft1">Prefetch</span></nobr></DIV>
<DIV style="position:absolute;top:971;left:203"><nobr><span class="ft1">PreconditionEnforcer</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:413"><nobr><span class="ft3">Ensures that any URIs which are preconditions for<br>the current URI are scheduled beforehand.</span></nobr></DIV>
<DIV style="position:absolute;top:1013;left:242"><nobr><span class="ft1">FetchDNS</span></nobr></DIV>
<DIV style="position:absolute;top:1003;left:413"><nobr><span class="ft3">Performs DNS lookups, for URIs of the “<i>dns:</i>”<br>scheme.</span></nobr></DIV>
</DIV>
<!-- Page 9 -->
<a name="9"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:122;left:238"><nobr><span class="ft1">FetchHTTP</span></nobr></DIV>
<DIV style="position:absolute;top:111;left:413"><nobr><span class="ft3">Performs HTTP retrievals, for URIs of the “<i>http:</i>”<br>and “<i>https:</i>” schemes.</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:221"><nobr><span class="ft1">ExtractorHTML</span></nobr></DIV>
<DIV style="position:absolute;top:154;left:413"><nobr><span class="ft1">Discovers URIs inside HTML resources.</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:238"><nobr><span class="ft1">ExtractorJS</span></nobr></DIV>
<DIV style="position:absolute;top:175;left:413"><nobr><span class="ft1">Discovers likely URIs inside Javascript resources.</span></nobr></DIV>
<DIV style="position:absolute;top:207;left:231"><nobr><span class="ft1">ExtractorCSS</span></nobr></DIV>
<DIV style="position:absolute;top:197;left:413"><nobr><span class="ft3">Discovers URIs inside Cascading Style Sheet<br>resources.</span></nobr></DIV>
<DIV style="position:absolute;top:250;left:228"><nobr><span class="ft1">ExtractorSWF</span></nobr></DIV>
<DIV style="position:absolute;top:239;left:413"><nobr><span class="ft3">Discovers URIs inside Shockwave/Flash<br>resources.</span></nobr></DIV>
<DIV style="position:absolute;top:292;left:230"><nobr><span class="ft1">ExtractorPDF</span></nobr></DIV>
<DIV style="position:absolute;top:281;left:413"><nobr><span class="ft3">Discovers URIs inside Adobe Portable Document<br>Format resources.</span></nobr></DIV>
<DIV style="position:absolute;top:334;left:228"><nobr><span class="ft1">ExtractorDOC</span></nobr></DIV>
<DIV style="position:absolute;top:324;left:413"><nobr><span class="ft3">Discovers URIs inside Microsoft Word document<br>resources.</span></nobr></DIV>
<DIV style="position:absolute;top:252;left:133" class="v"><nobr><span class="ft1">Extract</span></nobr></DIV>
<DIV style="position:absolute;top:376;left:212"><nobr><span class="ft1">ExtractorUniversal</span></nobr></DIV>
<DIV style="position:absolute;top:366;left:413"><nobr><span class="ft3">Discovers legal URI patterns inside any resource<br>with an ASCII-like encoding.</span></nobr></DIV>
<DIV style="position:absolute;top:428;left:133" class="v"><nobr><span class="ft1">Write</span></nobr></DIV>
<DIV style="position:absolute;top:440;left:203"><nobr><span class="ft1">ARCWriterProcessor</span></nobr></DIV>
<DIV style="position:absolute;top:430;left:413"><nobr><span class="ft3">Writes retrieved resources to a series of files in the<br>Internet Archive’s ARC file format.</span></nobr></DIV>
<DIV style="position:absolute;top:515;left:237"><nobr><span class="ft1">Postselector</span></nobr></DIV>
<DIV style="position:absolute;top:494;left:413"><nobr><span class="ft3">Evaluates URIs discovered by previous processors<br>against the configured crawl Scope, scheduling<br>those of interest to the Frontier.</span></nobr></DIV>
<DIV style="position:absolute;top:501;left:133" class="v"><nobr><span class="ft1">Postprocess</span></nobr></DIV>
<DIV style="position:absolute;top:568;left:211"><nobr><span class="ft1">CrawlStateUpdater</span></nobr></DIV>
<DIV style="position:absolute;top:557;left:413"><nobr><span class="ft3">Updates crawler-internal caches with new<br>information retrieved by earlier processors.</span></nobr></DIV>
<DIV style="position:absolute;top:657;left:135"><nobr><span class="ft0">Features and Limitations</span></nobr></DIV>
<DIV style="position:absolute;top:702;left:135"><nobr><span class="ft13">Features</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:135"><nobr><span class="ft1">Heritrix 1.0.0 offers the following key features:</span></nobr></DIV>
<DIV style="position:absolute;top:775;left:162"><nobr><span class="ft1">•  Collects content via HTTP recursively from multiple websites in a single crawl</span></nobr></DIV>
<DIV style="position:absolute;top:792;left:189"><nobr><span class="ft3">run, spanning hundreds to thousands of independent websites, and millions to tens<br>of millions of distinct resources, over a week or more of non-stop collection.</span></nobr></DIV>
<DIV style="position:absolute;top:837;left:162"><nobr><span class="ft1">•  Collects by site domains, exact host, or configurable URI patterns, starting from</span></nobr></DIV>
<DIV style="position:absolute;top:854;left:189"><nobr><span class="ft1">an operator-provided &quot;seed&quot; set of URIs</span></nobr></DIV>
<DIV style="position:absolute;top:878;left:162"><nobr><span class="ft1">•  Executes a primarily breadth-first, order-of-discovery policy for choosing URIs to</span></nobr></DIV>
<DIV style="position:absolute;top:896;left:189"><nobr><span class="ft3">process, with an option to prefer finishing sites in progress to beginning new sites<br>(“site-first” scheduling).</span></nobr></DIV>
<DIV style="position:absolute;top:941;left:162"><nobr><span class="ft1">•  Highly extensible with all of the major Heritrix components – the scheduling</span></nobr></DIV>
<DIV style="position:absolute;top:958;left:189"><nobr><span class="ft3">Frontier, the Scope, the protocol-based Fetch processors, filtering rules, format-<br>based Extract processors, content Write processors, and more – replaceable by<br>alternate implementations or extensions.  Documented APIs and HOW-TOs<br>explain extension options.</span></nobr></DIV>
<DIV style="position:absolute;top:1048;left:135"><nobr><span class="ft1">Highly configurable.  Options include:</span></nobr></DIV>
</DIV>
<!-- Page 10 -->
<a name="10"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft17{font-size:16px;line-height:28px;font-family:Times;color:#000000;}
	.ft18{font-size:18px;line-height:20px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:112;left:162"><nobr><span class="ft1">•  Settable output locations for logs, archive files, reports and temporary files.</span></nobr></DIV>
<DIV style="position:absolute;top:140;left:162"><nobr><span class="ft1">•  Settable maximum bytes to download, maximum number of documents to</span></nobr></DIV>
<DIV style="position:absolute;top:157;left:189"><nobr><span class="ft1">download, and maximum time to spend crawling.</span></nobr></DIV>
<DIV style="position:absolute;top:189;left:162"><nobr><span class="ft1">•  Settable number of 'worker' crawling threads.</span></nobr></DIV>
<DIV style="position:absolute;top:217;left:162"><nobr><span class="ft1">•  Settable upper bound on bandwidth-usage.</span></nobr></DIV>
<DIV style="position:absolute;top:246;left:162"><nobr><span class="ft1">•  Politeness configuration that allows setting minimum/maximum time between</span></nobr></DIV>
<DIV style="position:absolute;top:263;left:189"><nobr><span class="ft3">requests as well an option to base the lag between requests on a multiple of time<br>elapsed fulfilling the most recent request.</span></nobr></DIV>
<DIV style="position:absolute;top:307;left:162"><nobr><span class="ft1">•  Configurable inclusion/exclusion filtering mechanism.  Includes regular</span></nobr></DIV>
<DIV style="position:absolute;top:325;left:189"><nobr><span class="ft3">expression, URI path depth, and link hop count filters that can be combined<br>variously and attached at key points along the processing chain to enable fine<br>tuned inclusion/exclusion.</span></nobr></DIV>
<DIV style="position:absolute;top:394;left:135"><nobr><span class="ft3">Most options can be overridden on a per domain basis and then in turn further amended<br>based off time-of-day, a regular expression or URI port match 'refinements'.<br>Robots.txt refresh rate as well as rich URI-retry-on-failure configurations.<br>Web-based user interface (UI) –  “control panel” – via which crawls can be configured,<br>started, paused, stopped, and adjusted mid-crawl.  The UI can be used to view the current<br>state of the crawl, logs and generated reports.<br>Logs retrievals – the URI of the document downloaded, the download result code, time to<br>download, document size, and the link that pointed to the downloaded document – as<br>well as processing errors encountered fetching, scheduling and analyzing documents.<br>Reports capture the number of URIs visited, the number of URIs discovered and pending,<br>a summary of errors encountered, and memory/bandwidth/storage used – both in total<br>and on a sampling interval. Other runtime reports detail state of each crawl worker thread<br>and the state of the Frontier’s URI queues.<br>Includes link extractors for the common web document types -- HTML, CSS, JavaScript,<br>PDF, MS WORD, and FLASH – as well as a catchall “Universal Extractor” for<br>extracting links from everything else.<br>Archives the actual, binary DNS, HTTP, and HTTPS server responses, using an open<br>storage format (&quot;ARC&quot;).<br>Negotiation of &quot;http://www.faqs.org/rfcs/rfc2617.html&quot; RFC 2617 (BASIC and DIGEST<br>Authentication) and HTML form-based login authentication systems.<br>Written in portable Java, so works under Linux, Windows, Macintosh OS X, and other<br>systems.<br>All source code available under an open source license (Library/Lesser Gnu Public<br>License, LGPL).</span></nobr></DIV>
<DIV style="position:absolute;top:960;left:135"><nobr><span class="ft18">Limitations<br>Heritrix has been used primarily for doing focused crawls to date.  The broad and<br>continuous use cases are to be tackled in the next phase of development (see below).<br>Key current limitations to keep in mind are:</span></nobr></DIV>
<DIV style="position:absolute;top:1054;left:162"><nobr><span class="ft1">•  Single instance only: cannot coordinate crawling amongst multiple Heritrix</span></nobr></DIV>
</DIV>
<!-- Page 11 -->
<a name="11"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
	.ft19{font-size:21px;line-height:27px;font-family:Times;color:#000000;}
	.ft20{font-size:21px;line-height:22px;font-family:Times;color:#000000;}
-->
</STYLE>
<DIV style="position:absolute;top:109;left:189"><nobr><span class="ft3">instances whether all instances are run on a single machine or spread across<br>multiple machines.</span></nobr></DIV>
<DIV style="position:absolute;top:153;left:162"><nobr><span class="ft1">•  Requires sophisticated operator tuning to run large crawls within machine</span></nobr></DIV>
<DIV style="position:absolute;top:170;left:189"><nobr><span class="ft1">resource limits.</span></nobr></DIV>
<DIV style="position:absolute;top:195;left:162"><nobr><span class="ft3">•  Only officially supported and tested on Linux<br>•  Each crawl run is independent, without support for scheduled revisits to areas of</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:189"><nobr><span class="ft1">interest or incremental archival of changed material.</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:162"><nobr><span class="ft1">•  Limited ability to recover from in-crawl hardware/system failure.</span></nobr></DIV>
<DIV style="position:absolute;top:276;left:162"><nobr><span class="ft0">•  Minimal time spent profiling and optimizing has Heritrix coming up short on</span></nobr></DIV>
<DIV style="position:absolute;top:298;left:189"><nobr><span class="ft1">performance requirements (See Crawler Performance below).</span></nobr></DIV>
<DIV style="position:absolute;top:337;left:135"><nobr><span class="ft20">Crawler Performance<br>Currently the crawler is being used by Internet Archive to do several focused crawls for<br>our partners.  It is also being used by several of the Nordic country libraries for domain<br>crawling.  In each of these production crawls the Heritrix crawler is being tested in a<br>“true web” environment.  By performing weekly crawls, IA continuously tests the<br>capabilities of the crawler and monitors performance and quality compared to other<br>crawlers.  A typical weekly crawl configuration and administration is illustrated in on the<br>Heritrix crawler website at http://crawler.archive.org/cgi-bin/wiki.pl?HomePage.</span></nobr></DIV>
<DIV style="position:absolute;top:537;left:135"><nobr><span class="ft7">To evaluate the operational performance of the crawler Internet Archive measures the<br>crawl documents captured over time.  Typical performance of the Heritrix crawler for<br>crawls spanning over several days to several weeks are illustrated in the chart below.</span></nobr></DIV>
<DIV style="position:absolute;top:614;left:135"><nobr><span class="ft7">Chart 1 demonstrates the performance of the crawler during a small ( less than 20 seeds)<br>focused crawl and a much broader crawl(several hundred seeds).  For the small focused<br>crawl the rate of documents discovered quickly drops off and slows the crawler as the<br>discovery process is completed.  For the larger crawl (line 2) the rate of documents<br>discovered over time remains unchanged as the discovery process continues over time.<br>This is the type of behavior one would expect for a broad crawl if there is always a queue<br>of URIs in the frontier.</span></nobr></DIV>
<DIV style="position:absolute;top:780;left:135"><nobr><span class="ft1">Chart 1  Documents discovered per second over time</span></nobr></DIV>
</DIV>
<!-- Page 12 -->
<a name="12"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:490;left:135"><nobr><span class="ft7">A set of tools was developed at the Internet Archive for internal use to determine the<br>quality of a crawl compared to other independent crawlers or between periodic crawls.<br>The URI comparison tool is a post-crawling analysis tool that compares two sets of URIs<br>and produces statistical reports on similarity and the differences between two crawls.<br>This tool can be used to compare two crawls having the same URI seeds or to compare<br>coverage of different crawling software.</span></nobr></DIV>
<DIV style="position:absolute;top:634;left:135"><nobr><span class="ft7">The URI comparison tool requires input of two crawl sets of URIs and their respective<br>HTTP response codes, content lengths, and MIME types. When two different crawls,</span></nobr></DIV>
<DIV style="position:absolute;top:688;left:135"><nobr><span class="ft7">When using the comparison tools, one can determine the percent overlap between two<br>crawls and what percentage of content is unique to both crawls.</span></nobr></DIV>
<DIV style="position:absolute;top:742;left:135"><nobr><span class="ft7">This tool can also be used to measure URI overlap between periodic crawls (ie weekly or<br>daily) starting from the same seed list.  The chart below shows for this particularly<br>weekly crawl this is a 70% overlap of identical URI’s.  In this particular case where the<br>overlap is so high one would want to employ deduplication techniques if plausible.  This<br>chart also shows the number of new URI’s discovered each week and number of URIs<br>which have disappeared from the prior week.</span></nobr></DIV>
<DIV style="position:absolute;top:918;left:135"><nobr><span class="ft1">Chart 2 Weekly comparision Crawl of 15 seed domains</span></nobr></DIV>
</DIV>
<!-- Page 13 -->
<a name="13"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:487;left:135"><nobr><span class="ft7">Many of these tests and quality assurance procedures are still in development as we<br>continue to learn more about quality archival crawling.</span></nobr></DIV>
<DIV style="position:absolute;top:580;left:135"><nobr><span class="ft2">Future Plans<br>Current plans for the future development of Heritrix, post 1.0.0, fall into three general<br>categories: improving its scale of operation; adding advanced scheduling options; and<br>incremental modular extension.</span></nobr></DIV>
<DIV style="position:absolute;top:694;left:135"><nobr><span class="ft3">Our top priority at the Internet Archive is to dramatically increase the scale of crawls<br>possible with Heritrix, into crawls that span hundreds of millions and then billions of web<br>resources. This entails two necessary advances.</span></nobr></DIV>
<DIV style="position:absolute;top:777;left:135"><nobr><span class="ft3">First, the ability to perform crawls of arbitrarily large size and duration on a single<br>machine, with minimal operator tuning, limited only by available disk storage. This will<br>require changes that strictly cap the RAM used by the crawler, and the conversion of<br>every crawler data structure that grows with crawl size or duration to use disk-based data<br>structures, trading away performance for long, large runs.</span></nobr></DIV>
<DIV style="position:absolute;top:901;left:135"><nobr><span class="ft3">Second, a way to arrange for networks of cooperating crawl machines to intelligently<br>divide up the work of a crawl. This will enable the rapid acquisition of ever-larger<br>collections of web resources, limited only by the bandwidth and local hardware resources<br>devoted to the crawl. Enabling effective distribution across machines requires strategies<br>for dividing the work, sharing results, and recovering from partial failures. Other work in<br>parallel crawling clusters, such as that done by the UbiCrawler project, provides useful<br>precedent for the necessary additions.</span></nobr></DIV>
</DIV>
<!-- Page 14 -->
<a name="14"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:109;left:135"><nobr><span class="ft3">Separately, there are a number of incremental feature improvements planned to expand<br>the base crawler capabilities and the range of optional components available. The Internet<br>Archive plans to implement:</span></nobr></DIV>
<DIV style="position:absolute;top:174;left:162"><nobr><span class="ft3">•  Support for FTP fetching<br>•  Improved recovery of crawls from set checkpoints<br>•  Automatic detection and adaptation to many “crawler traps” and challenging</span></nobr></DIV>
<DIV style="position:absolute;top:233;left:189"><nobr><span class="ft1">website design practices (such as URI-line session-IDs)</span></nobr></DIV>
<DIV style="position:absolute;top:257;left:162"><nobr><span class="ft1">•  Additional operator options for specifying crawl scopes, dealing with in-crawl</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:189"><nobr><span class="ft1">problems, and forcing the retry of groups of URIs.</span></nobr></DIV>
<DIV style="position:absolute;top:320;left:135"><nobr><span class="ft13">Collaboration with other Institutions</span></nobr></DIV>
<DIV style="position:absolute;top:358;left:135"><nobr><span class="ft3">A top priority of several outside efforts building atop Heritrix is to add greater<br>sophistication to the scheduling of URIs.  This will likely include:</span></nobr></DIV>
<DIV style="position:absolute;top:423;left:162"><nobr><span class="ft3">•  Periodic revisits of sites or URIs at predetermined intervals<br>•  Working to only fetch, or only store, changed resources<br>•  Adaptively predicting rates of change and using such information to change</span></nobr></DIV>
<DIV style="position:absolute;top:482;left:189"><nobr><span class="ft1">revisit schedules</span></nobr></DIV>
<DIV style="position:absolute;top:506;left:162"><nobr><span class="ft1">•  Synamically prioritizing work based on diverse measures of site and URI value</span></nobr></DIV>
<DIV style="position:absolute;top:544;left:135"><nobr><span class="ft3">Discussions are underway for such work to be coordinated with interested European<br>National Libraries.</span></nobr></DIV>
<DIV style="position:absolute;top:611;left:135"><nobr><span class="ft13">In the open source community</span></nobr></DIV>
<DIV style="position:absolute;top:649;left:135"><nobr><span class="ft3">Others are using the crawler outside of the archiving and library community.  While<br>anyone can download Heritrix with no obligation to report back on its use or extension,<br>questions and patches coming in to the general Heritrix discussion list will sometimes<br>comment on the projects to which Heritrix is being applied.  One group uses Heritrix to<br>harvest web pages as a means of testing whether their commercial tokenizing tools<br>continue to work against the latest web content.  Another group is using Heritrix to take<br>frequent snapshots of particular web servers for a University research project.  It appears<br>someone else is investigating Heritrix as a way to harvest institutional contact addresses,<br>perhaps to make unsolicited commercial approaches. (We have no veto power over<br>outside uses of our released software.)</span></nobr></DIV>
<DIV style="position:absolute;top:876;left:135"><nobr><span class="ft3">A main focus for the future is continued fostering of the community that is growing up<br>around Heritrix, to help improve the common code base with diverse crawling experience<br>and enhancements.</span></nobr></DIV>
<DIV style="position:absolute;top:1023;left:135"><nobr><span class="ft4">Conclusion<br>The Internet Archive’s digital collections include a large and unique historical record of</span></nobr></DIV>
</DIV>
<!-- Page 15 -->
<a name="15"></a>
<DIV style="position:relative;width:918;height:1188;">
<STYLE type="text/css">
<!--
-->
</STYLE>
<DIV style="position:absolute;top:109;left:135"><nobr><span class="ft3">web content. This Web collection continues to grow each year, with regular Alexa<br>snapshots and now, material collected by the new Heritrix web crawling software.<br>Heritrix software is the product of an open source approach and international<br>collaboration. In its 1.0.0 version, Heritrix is a full-featured focused crawler with a<br>flexible architecture to enable many divergent crawling practices, now and in the future.<br>Its evolution should continue to benefit from an open collaborative process, as it becomes<br>capable of larger and more sophisticated crawling tasks.</span></nobr></DIV>
<DIV style="position:absolute;top:274;left:135"><nobr><span class="ft1">Acknowledgements</span></nobr></DIV>
<DIV style="position:absolute;top:316;left:135"><nobr><span class="ft3">This paper includes significant contributions from others at the Internet Archive,<br>including Brewster Kahle, Raymie Stata and Brad Tofel,</span></nobr></DIV>
<DIV style="position:absolute;top:377;left:135"><nobr><span class="ft1">References</span></nobr></DIV>
<DIV style="position:absolute;top:419;left:135"><nobr><span class="ft3">[GUTENBERG] http://www.gutenberg.net/<br>[ICDL] http://www.icdlbooks.org/<br>[IIPC] http://www.netpreserve.org/<br>[ALEXA] http://pages.alexa.com/company/index.html<br>[BURNER97] http://www.webtechniques.com/archives/1997/05/burner/<br>[WAYBACK] http://www.archive.org/web/web.php<br>[NWA] http://nwa.nb.no/<br>[SOURCEFORGE] http://www.sourceforge.net<br>[LGPL] http://www.gnu.org/copyleft/lesser.html<br>[HTTRACK] http://www.httrack.com<br>[MERCATOR] http://research.compaq.com/SRC/mercator/<br>[JETTY] http://jetty.mortbay.com/<br>[ROBOTS] http://www.robotstxt.org<br>[UBICRAWLER] http://ubi.imc.pi.cnr.it/projects/ubicrawler/</span></nobr></DIV>
</DIV>
</BODY>
</HTML>
